{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# In this Demo, we will:\n",
        "\n",
        "1. Download ggml-whisper which is a really fast version of whisper written in C/C++\n",
        "2. Convert an audio file with any format to wav (the only format currently supported by ggml-whisper)\n",
        "3. Convert the transcription to csv and save for model training later"
      ],
      "metadata": {
        "id": "uLih1l-Xr4FD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e27e-ve03eyh"
      },
      "source": [
        "## Download an example video"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "F7n_NfZcuszO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b5771d-782a-4d5a-f2b3-50513720ef3b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogcy9q2V3jjK"
      },
      "source": [
        "## Download ggml-whisper repo and compile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/whisper.cpp.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThvylUNhFoz8",
        "outputId": "6b912cad-c989-48a6-d869-f61b28f9a683"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'whisper.cpp'...\n",
            "remote: Enumerating objects: 4091, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 4091 (delta 29), reused 47 (delta 15), pack-reused 4015\u001b[K\n",
            "Receiving objects: 100% (4091/4091), 7.00 MiB | 10.68 MiB/s, done.\n",
            "Resolving deltas: 100% (2552/2552), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/whisper.cpp/models\n",
        "!bash download-ggml-model.sh base.en\n",
        "%cd ..\n",
        "!make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd3JnpQojLD_",
        "outputId": "d97d2d35-3089-4e41-b073-56a58eb12be1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/whisper.cpp/models\n",
            "Downloading ggml model base.en from 'https://huggingface.co/ggerganov/whisper.cpp' ...\n",
            "ggml-base.en.bin    100%[===================>] 141.11M   204MB/s    in 0.7s    \n",
            "Done! Model 'base.en' saved in 'models/ggml-base.en.bin'\n",
            "You can now use it like this:\n",
            "\n",
            "  $ ./main -m models/ggml-base.en.bin -f samples/jfk.wav\n",
            "\n",
            "/content/whisper.cpp\n",
            "I whisper.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx2 -mfma -mf16c -mavx -msse3\n",
            "I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:      g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx2 -mfma -mf16c -mavx -msse3   -c ggml.c -o ggml.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread -c whisper.cpp -o whisper.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread examples/main/main.cpp examples/common.cpp examples/common-ggml.cpp ggml.o whisper.o -o main \n",
            "./main -h\n",
            "\n",
            "usage: ./main [options] file0.wav file1.wav ...\n",
            "\n",
            "options:\n",
            "  -h,        --help              [default] show this help message and exit\n",
            "  -t N,      --threads N         [2      ] number of threads to use during computation\n",
            "  -p N,      --processors N      [1      ] number of processors to use during computation\n",
            "  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n",
            "  -on N,     --offset-n N        [0      ] segment index offset\n",
            "  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n",
            "  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n",
            "  -ml N,     --max-len N         [0      ] maximum segment length in characters\n",
            "  -sow,      --split-on-word     [false  ] split on word rather than on token\n",
            "  -bo N,     --best-of N         [2      ] number of best candidates to keep\n",
            "  -bs N,     --beam-size N       [-1     ] beam size for beam search\n",
            "  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n",
            "  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n",
            "  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n",
            "  -su,       --speed-up          [false  ] speed up audio by x2 (reduced accuracy)\n",
            "  -tr,       --translate         [false  ] translate from source language to english\n",
            "  -di,       --diarize           [false  ] stereo audio diarization\n",
            "  -tdrz,     --tinydiarize       [false  ] enable tinydiarize (requires a tdrz model)\n",
            "  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n",
            "  -otxt,     --output-txt        [false  ] output result in a text file\n",
            "  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n",
            "  -osrt,     --output-srt        [false  ] output result in a srt file\n",
            "  -olrc,     --output-lrc        [false  ] output result in a lrc file\n",
            "  -owts,     --output-words      [false  ] output script for generating karaoke video\n",
            "  -fp,       --font-path         [/System/Library/Fonts/Supplemental/Courier New Bold.ttf] path to a monospace font for karaoke video\n",
            "  -ocsv,     --output-csv        [false  ] output result in a CSV file\n",
            "  -oj,       --output-json       [false  ] output result in a JSON file\n",
            "  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n",
            "  -ps,       --print-special     [false  ] print special tokens\n",
            "  -pc,       --print-colors      [false  ] print colors\n",
            "  -pp,       --print-progress    [false  ] print progress\n",
            "  -nt,       --no-timestamps     [false  ] do not print timestamps\n",
            "  -l LANG,   --language LANG     [en     ] spoken language ('auto' for auto-detect)\n",
            "  -dl,       --detect-language   [false  ] exit after automatically detecting language\n",
            "             --prompt PROMPT     [       ] initial prompt\n",
            "  -m FNAME,  --model FNAME       [models/ggml-base.en.bin] model path\n",
            "  -f FNAME,  --file FNAME        [       ] input WAV file path\n",
            "  -oved D,   --ov-e-device DNAME [CPU    ] the OpenVINO device used for encode inference\n",
            "\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread examples/bench/bench.cpp ggml.o whisper.o -o bench \n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread examples/quantize/quantize.cpp examples/common.cpp examples/common-ggml.cpp ggml.o whisper.o -o quantize \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run ggml-whisper on sample file"
      ],
      "metadata": {
        "id": "1DxBf3MWj8Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that main example currently runs only with 16-bit WAV files,\n",
        "# convert input before running the tool.'\n",
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/audio_3ai2\"\n",
        "i = 0\n",
        "print(path)\n",
        "# Loop through the files in the directory\n",
        "for file in os.listdir(path):\n",
        "    if file.endswith(\".mp3\"):  # Process only MP3 files\n",
        "        i += 1\n",
        "        print(file)\n",
        "\n",
        "        # Convert the MP3 to WAV using FFmpeg\n",
        "        input_file = os.path.join(path, file)\n",
        "        output_file = os.path.join(path, f\"wav_files/{i}.wav\")\n",
        "        !ffmpeg -i \"$input_file\" -ar 16000 -ac 1 -c:a pcm_s16le \"$output_file\"\n",
        "\n",
        "        # Process the WAV file using the \"main\" program\n",
        "        !./main -f \"$output_file\"\n",
        "\n"
      ],
      "metadata": {
        "id": "u7X746feDK2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Redirect and save transcription output and transform to csv"
      ],
      "metadata": {
        "id": "8YPlXB9im24p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# path = \"/content/drive/MyDrive/audio_3ai2\"\n",
        "# wav_folder = os.path.join(path, f\"wav_files\")\n",
        "# i=0\n",
        "# for wavf in wav_folder:\n",
        "#   i=i+1\n",
        "#   dest = os.path.join(path, f\"transcripts/{i}.txt\")\n",
        "#   !./main -f wavf > dest\n",
        "\n",
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/audio_3ai2\"\n",
        "wav_folder = os.path.join(path, \"wav_files\")\n",
        "\n",
        "# Create the transcripts folder if it doesn't exist\n",
        "transcripts_folder = os.path.join(path, \"transcripts\")\n",
        "os.makedirs(transcripts_folder, exist_ok=True)\n",
        "\n",
        "# Iterate through WAV files in the folder\n",
        "for i, wav_file in enumerate(os.listdir(wav_folder)):\n",
        "    if wav_file.endswith('.wav'):\n",
        "        wav_file_path = os.path.join(wav_folder, wav_file)\n",
        "        dest = os.path.join(transcripts_folder, f\"{i+1}.txt\")\n",
        "\n",
        "        # Run the command to generate transcripts using ./main\n",
        "        command = f\"./main -f '{wav_file_path}' > '{dest}'\"\n",
        "        os.system(command)\n",
        "\n",
        "print(\"Transcripts generated and saved.\")\n"
      ],
      "metadata": {
        "id": "NkaGH8vbz9G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "def generate_transcription_csv(folder_path):\n",
        "    # Specify the transcripts folder\n",
        "    transcript_folder = os.path.join(folder_path, 'transcripts')\n",
        "\n",
        "    # Specify the csv_files folder\n",
        "    csv_folder = os.path.join(folder_path, 'csv_files')\n",
        "\n",
        "    # Create the csv_files folder if it doesn't exist\n",
        "    os.makedirs(csv_folder, exist_ok=True)\n",
        "\n",
        "    # Iterate through transcript files in the \"transcripts\" folder\n",
        "    for transcript_file in os.listdir(transcript_folder):\n",
        "        if transcript_file.endswith('.txt'):\n",
        "            csv_file_name = os.path.splitext(transcript_file)[0] + '.csv'\n",
        "            csv_file_path = os.path.join(csv_folder, csv_file_name)\n",
        "\n",
        "            with open(csv_file_path, 'w') as csv_file:\n",
        "                csv_writer = csv.writer(csv_file)\n",
        "                csv_writer.writerow(['time_stamp', 'transcription'])  # Writing header\n",
        "\n",
        "                transcript_path = os.path.join(transcript_folder, transcript_file)\n",
        "                with open(transcript_path, 'r') as txt_file:\n",
        "                    for line in txt_file:\n",
        "                        line = line.strip()\n",
        "                        print(\"in loop\")\n",
        "                        # Skip empty lines\n",
        "                        if line == '':\n",
        "                            continue\n",
        "\n",
        "                        # Split the line into timestamp and transcription\n",
        "                        time_stamp, transcription = line.split(']', 1)\n",
        "\n",
        "                        # Remove the leading '[' from the timestamp\n",
        "                        time_stamp = time_stamp[1:]\n",
        "\n",
        "                        # Remove leading and trailing spaces from the transcription\n",
        "                        transcription = transcription.strip()\n",
        "\n",
        "                        # Skip lines where the transcription is enclosed in square brackets\n",
        "                        if not (transcription.startswith('[') and transcription.endswith(']')):\n",
        "                            # Write the data to the CSV file\n",
        "                            csv_writer.writerow([time_stamp, transcription])\n",
        "\n",
        "            print(f\"CSV file has been saved at: {csv_file_path}\")\n",
        "\n",
        "\n",
        "# Specify the main folder path\n",
        "main_folder_path = \"/content/drive/MyDrive/audio_3ai2\"\n",
        "\n",
        "# Call the function to generate CSV files from transcripts\n",
        "generate_transcription_csv(main_folder_path)\n"
      ],
      "metadata": {
        "id": "tja__SBZ2_G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modularize and put everything together"
      ],
      "metadata": {
        "id": "2UWMIgQ9vD3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/whisper.cpp.git\n",
        "\n",
        "%cd /content/whisper.cpp/models\n",
        "!bash download-ggml-model.sh base.en\n",
        "%cd ..\n",
        "!make\n",
        "\n",
        "!pip install pydub\n",
        "!apt install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhhnSa-lvDAt",
        "outputId": "0a22202c-39cc-4bbe-b175-952e619c7e21"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'whisper.cpp' already exists and is not an empty directory.\n",
            "/content/whisper.cpp/models\n",
            "Downloading ggml model base.en from 'https://huggingface.co/ggerganov/whisper.cpp' ...\n",
            "Model base.en already exists. Skipping download.\n",
            "/content/whisper.cpp\n",
            "I whisper.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx2 -mfma -mf16c -mavx -msse3\n",
            "I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:      g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "make: Nothing to be done for 'default'.\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add DOC_ID Column"
      ],
      "metadata": {
        "id": "mDm8qUGTi6Kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def add_ids_to_csv_files(folder_path):\n",
        "    # Iterate through CSV files in the folder\n",
        "    csv_folder = os.path.join(folder_path, 'csv_files')\n",
        "    for csv_file in os.listdir(csv_folder):\n",
        "        if csv_file.endswith('.csv'):\n",
        "            csv_file_path = os.path.join(csv_folder, csv_file)\n",
        "\n",
        "            # Read the CSV file\n",
        "            df = pd.read_csv(csv_file_path)\n",
        "\n",
        "            # Add the new column with values from 0 to n-1\n",
        "            df['DOC_ID'] = range(len(df))\n",
        "\n",
        "            # Save the updated DataFrame back to the CSV file\n",
        "            df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "            print(f\"Updated {csv_file_path} with IDs.\")\n",
        "\n",
        "\n",
        "# Specify the main folder path\n",
        "main_folder_path = \"/content/drive/MyDrive/audio_3ai2\"\n",
        "\n",
        "# Call the add_ids_to_csv_files function to process all CSV files in the folder\n",
        "add_ids_to_csv_files(main_folder_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4WxTCCJMYMW",
        "outputId": "5e610cc6-1ee4-4fa8-da87-07c7ced1829b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated /content/drive/MyDrive/audio_3ai2/csv_files/1.csv with IDs.\n",
            "Updated /content/drive/MyDrive/audio_3ai2/csv_files/2.csv with IDs.\n",
            "Updated /content/drive/MyDrive/audio_3ai2/csv_files/3.csv with IDs.\n",
            "Updated /content/drive/MyDrive/audio_3ai2/csv_files/4.csv with IDs.\n",
            "Updated /content/drive/MyDrive/audio_3ai2/csv_files/5.csv with IDs.\n",
            "Updated /content/drive/MyDrive/audio_3ai2/csv_files/6.csv with IDs.\n",
            "Updated /content/drive/MyDrive/audio_3ai2/csv_files/7.csv with IDs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install thirdai --upgrade\n",
        "!pip3 install thirdai[neural_db]\n",
        "!pip3 install langchain --upgrade\n",
        "!pip3 install openai --upgrade\n",
        "!pip3 install paper-qa --upgrade"
      ],
      "metadata": {
        "id": "ZhlMc2bY_9oS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9702143c-feeb-4a61-f660-8502d7a4319d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting thirdai\n",
            "  Downloading thirdai-0.7.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from thirdai) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from thirdai) (4.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from thirdai) (2.27.1)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from thirdai) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->thirdai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->thirdai) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai) (3.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->thirdai) (1.16.0)\n",
            "Installing collected packages: thirdai\n",
            "Successfully installed thirdai-0.7.16\n",
            "Requirement already satisfied: thirdai[neural_db] in /usr/local/lib/python3.10/dist-packages (0.7.16)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (4.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (2.27.1)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (1.5.3)\n",
            "Collecting PyTrie (from thirdai[neural_db])\n",
            "  Downloading PyTrie-0.4.0.tar.gz (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyMuPDF (from thirdai[neural_db])\n",
            "  Downloading PyMuPDF-1.22.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain (from thirdai[neural_db])\n",
            "  Downloading langchain-0.0.252-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bs4 (from thirdai[neural_db])\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting trafilatura (from thirdai[neural_db])\n",
            "  Downloading trafilatura-1.6.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-docx (from thirdai[neural_db])\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting url-normalize (from thirdai[neural_db])\n",
            "  Downloading url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (3.8.1)\n",
            "Collecting unidecode (from thirdai[neural_db])\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (1.10.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->thirdai[neural_db]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->thirdai[neural_db]) (2022.7.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->thirdai[neural_db]) (4.11.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->thirdai[neural_db]) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->thirdai[neural_db]) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->thirdai[neural_db]) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->thirdai[neural_db]) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain->thirdai[neural_db])\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.11 (from langchain->thirdai[neural_db])\n",
            "  Downloading langsmith-0.0.19-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain->thirdai[neural_db]) (2.8.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain->thirdai[neural_db])\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->thirdai[neural_db]) (8.2.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai[neural_db]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai[neural_db]) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai[neural_db]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai[neural_db]) (3.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->thirdai[neural_db]) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->thirdai[neural_db]) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->thirdai[neural_db]) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->thirdai[neural_db]) (4.65.0)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx->thirdai[neural_db]) (4.9.3)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from PyTrie->thirdai[neural_db]) (2.4.0)\n",
            "Collecting courlan>=0.9.3 (from trafilatura->thirdai[neural_db])\n",
            "  Downloading courlan-0.9.3-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting htmldate>=1.4.3 (from trafilatura->thirdai[neural_db])\n",
            "  Downloading htmldate-1.4.3-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting justext>=3.0.0 (from trafilatura->thirdai[neural_db])\n",
            "  Downloading jusText-3.0.0-py2.py3-none-any.whl (837 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.8/837.8 kB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of trafilatura to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting trafilatura (from thirdai[neural_db])\n",
            "  Downloading trafilatura-1.6.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading trafilatura-1.5.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading trafilatura-1.4.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading trafilatura-1.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading trafilatura-1.3.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading trafilatura-1.2.2-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from url-normalize->thirdai[neural_db]) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai[neural_db]) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai[neural_db]) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai[neural_db]) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai[neural_db]) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai[neural_db]) (1.3.1)\n",
            "Requirement already satisfied: langcodes>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from courlan>=0.9.3->trafilatura->thirdai[neural_db]) (3.3.0)\n",
            "Collecting tld>=0.13 (from courlan>=0.9.3->trafilatura->thirdai[neural_db])\n",
            "  Downloading tld-0.13-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.8/263.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain->thirdai[neural_db])\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain->thirdai[neural_db])\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting dateparser>=1.1.2 (from htmldate>=1.4.3->trafilatura->thirdai[neural_db])\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of htmldate to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting htmldate>=1.2.1 (from trafilatura->thirdai[neural_db])\n",
            "  Downloading htmldate-1.4.2-py3-none-any.whl (33 kB)\n",
            "  Downloading htmldate-1.4.1-py3-none-any.whl (33 kB)\n",
            "  Downloading htmldate-1.4.0-py3-none-any.whl (33 kB)\n",
            "  Downloading htmldate-1.3.2-py3-none-any.whl (39 kB)\n",
            "  Downloading htmldate-1.3.1-py3-none-any.whl (39 kB)\n",
            "  Downloading htmldate-1.3.0-py3-none-any.whl (32 kB)\n",
            "  Downloading htmldate-1.2.3-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->thirdai[neural_db]) (2.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->thirdai[neural_db]) (2.4.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser>=1.1.2->htmldate>=1.4.3->trafilatura->thirdai[neural_db]) (5.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain->thirdai[neural_db]) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->thirdai[neural_db])\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: bs4, python-docx, PyTrie\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=b05099f4f529f1444a641b985cb06fcb4f9f46d01537f193cff1dc36b3ab5941\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184489 sha256=0bc733113180a5a41fd986b1ff775674d4f5fa84b9ee37e5e4b0df07ee5331e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "  Building wheel for PyTrie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyTrie: filename=PyTrie-0.4.0-py3-none-any.whl size=6080 sha256=50c026eb46274491844a26893fb284323897b22c1f274bae92904ecf71d7adf0\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/0e/a3/3563272cb57af4afbc50c4c7882dd4540944aadde25c82bd45\n",
            "Successfully built bs4 python-docx PyTrie\n",
            "Installing collected packages: url-normalize, unidecode, tld, PyTrie, python-docx, PyMuPDF, mypy-extensions, marshmallow, justext, typing-inspect, openapi-schema-pydantic, langsmith, dateparser, courlan, bs4, htmldate, dataclasses-json, trafilatura, langchain\n",
            "Successfully installed PyMuPDF-1.22.5 PyTrie-0.4.0 bs4-0.0.1 courlan-0.9.3 dataclasses-json-0.5.14 dateparser-1.1.8 htmldate-1.2.3 justext-3.0.0 langchain-0.0.252 langsmith-0.0.19 marshmallow-3.20.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 python-docx-0.8.11 tld-0.13 trafilatura-1.2.2 typing-inspect-0.9.0 unidecode-1.3.6 url-normalize-1.4.3\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.252)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.14)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.19)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n",
            "Collecting paper-qa\n",
            "  Downloading paper_qa-3.5.0-py3-none-any.whl (29 kB)\n",
            "Collecting pypdf (from paper-qa)\n",
            "  Downloading pypdf-3.14.0-py3-none-any.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.8/269.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain>=0.0.198 in /usr/local/lib/python3.10/dist-packages (from paper-qa) (0.0.252)\n",
            "Requirement already satisfied: openai>=0.27.8 in /usr/local/lib/python3.10/dist-packages (from paper-qa) (0.27.8)\n",
            "Collecting faiss-cpu (from paper-qa)\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyCryptodome (from paper-qa)\n",
            "  Downloading pycryptodome-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting html2text (from paper-qa)\n",
            "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Collecting tiktoken>=0.4.0 (from paper-qa)\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (0.5.14)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (0.0.19)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (1.22.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (8.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai>=0.27.8->paper-qa) (4.65.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.4.0->paper-qa) (2022.10.31)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain>=0.0.198->paper-qa) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.198->paper-qa) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.198->paper-qa) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.198->paper-qa) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.198->paper-qa) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (1.0.0)\n",
            "Installing collected packages: faiss-cpu, pypdf, PyCryptodome, html2text, tiktoken, paper-qa\n",
            "Successfully installed PyCryptodome-3.18.0 faiss-cpu-1.7.4 html2text-2020.1.16 paper-qa-3.5.0 pypdf-3.14.0 tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from thirdai import licensing, neural_db as ndb\n",
        "licensing.deactivate()\n",
        "licensing.activate(\"1FB7DD-CAC3EC-832A67-84208D-C4E39E-V3\")"
      ],
      "metadata": {
        "id": "LP1Np_5tEBay"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = ndb.NeuralDB(user_id=\"root\")"
      ],
      "metadata": {
        "id": "5C4CrYQXFC1n"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "# from pandas_gbq import ndb\n",
        "\n",
        "def process_csv_files(folder_path):\n",
        "    insertable_docs = []\n",
        "\n",
        "    # Iterate through CSV files in the folder\n",
        "    csv_folder = os.path.join(folder_path, 'csv_files')\n",
        "    for csv_file in os.listdir(csv_folder):\n",
        "        if csv_file.endswith('.csv'):\n",
        "            csv_file_path = os.path.join(csv_folder, csv_file)\n",
        "\n",
        "            # Read the CSV file\n",
        "            df = pd.read_csv(csv_file_path)\n",
        "\n",
        "            # Create a CSV document\n",
        "            csv_doc = ndb.CSV(\n",
        "                path=csv_file_path,\n",
        "                id_column=\"DOC_ID\",\n",
        "                strong_columns=[\"transcription\"],\n",
        "                weak_columns=[\"time_stamp\"],\n",
        "                reference_columns=[\"time_stamp\"])\n",
        "\n",
        "            insertable_docs.append(csv_doc)\n",
        "\n",
        "    return insertable_docs\n",
        "\n",
        "\n",
        "# Specify the main folder path\n",
        "main_folder_path = \"/content/drive/MyDrive/audio_3ai2\"\n",
        "\n",
        "# Call the process_csv_files function to process all CSV files in the folder\n",
        "insertable_docs = process_csv_files(main_folder_path)\n",
        "\n",
        "# Now you can use the insertable_docs list for further processing\n"
      ],
      "metadata": {
        "id": "cOtjQTGXNQN2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_ids = db.insert(insertable_docs, train=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuL_CHCkOvEE",
        "outputId": "df53a3fb-5a9f-45e9-f421-073d51c4704d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded data | source 'Documents:\n",
            "1.csv\n",
            "2.csv\n",
            "3.csv\n",
            "3.csv\n",
            "5.csv\n",
            "6.csv\n",
            "7.csv' | vectors 4922 | batches 3 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 3 | train_hash_precision@5=0.0609102  | train_batches 3 | time 30s\n",
            "\n",
            "train | epoch 1 | train_steps 6 | train_hash_precision@5=0.067371  | train_batches 3 | time 20s\n",
            "\n",
            "train | epoch 2 | train_steps 9 | train_hash_precision@5=0.0905729  | train_batches 3 | time 18s\n",
            "\n",
            "train | epoch 3 | train_steps 12 | train_hash_precision@5=0.15384  | train_batches 3 | time 18s\n",
            "\n",
            "train | epoch 4 | train_steps 15 | train_hash_precision@5=0.237017  | train_batches 3 | time 18s\n",
            "\n",
            "train | epoch 5 | train_steps 18 | train_hash_precision@5=0.353149  | train_batches 3 | time 20s\n",
            "\n",
            "train | epoch 6 | train_steps 21 | train_hash_precision@5=0.505729  | train_batches 3 | time 21s\n",
            "\n",
            "train | epoch 7 | train_steps 24 | train_hash_precision@5=0.653434  | train_batches 3 | time 19s\n",
            "\n",
            "train | epoch 8 | train_steps 27 | train_hash_precision@5=0.76282  | train_batches 3 | time 18s\n",
            "\n",
            "train | epoch 9 | train_steps 30 | train_hash_precision@5=0.85514  | train_batches 3 | time 18s\n",
            "\n",
            "train | epoch 10 | train_steps 33 | train_hash_precision@5=0.897074  | train_batches 3 | time 18s\n",
            "\n",
            "train | epoch 11 | train_steps 36 | train_hash_precision@5=0.945226  | train_batches 3 | time 19s\n",
            "\n",
            "train | epoch 12 | train_steps 39 | train_hash_precision@5=0.975254  | train_batches 3 | time 20s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_results = db.search(\n",
        "    query=\"Andrew Ng thoughts about deep learning\",\n",
        "    top_k=4,\n",
        "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
        "\n",
        "for result in search_results:\n",
        "    print(result.text)\n",
        "    print(result.context(radius=1))\n",
        "    print(result.source)\n",
        "    print(result.metadata)\n",
        "    print('************')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0L1bb92OEtY",
        "outputId": "abf32177-4201-4cb7-c73e-5c55db8a4589"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "00:34:33.600 --> 00:34:40.160\n",
            "00:34:29.440 --> 00:34:33.600 00:34:33.600 --> 00:34:40.160 00:34:40.720 --> 00:34:48.080\n",
            "/content/drive/MyDrive/audio_3ai2/csv_files/1.csv\n",
            "{'time_stamp': '00:34:33.600 --> 00:34:40.160', 'transcription': 'So how does one get started in deep learning and where does deep learning.ai fit into that?', 'DOC_ID': 362}\n",
            "************\n",
            "00:04:31.840 --> 00:04:33.280\n",
            "00:04:28.680 --> 00:04:31.840 00:04:31.840 --> 00:04:33.280 00:04:33.280 --> 00:04:36.640\n",
            "/content/drive/MyDrive/audio_3ai2/csv_files/7.csv\n",
            "{'time_stamp': '00:04:31.840 --> 00:04:33.280', 'transcription': 'And I thought, this is going to be great.', 'DOC_ID': 87}\n",
            "************\n",
            "01:17:28.640 --> 01:17:36.400\n",
            "01:17:22.480 --> 01:17:28.640 01:17:28.640 --> 01:17:36.400 01:17:36.400 --> 01:17:41.760\n",
            "/content/drive/MyDrive/audio_3ai2/csv_files/1.csv\n",
            "{'time_stamp': '01:17:28.640 --> 01:17:36.400', 'transcription': 'And the machine learning person says, no, wait, I did well on the test set. And I think there is a', 'DOC_ID': 809}\n",
            "************\n",
            "00:04:08.480 --> 00:04:13.280\n",
            "00:04:03.040 --> 00:04:08.480 00:04:08.480 --> 00:04:13.280 00:04:13.280 --> 00:04:18.800\n",
            "/content/drive/MyDrive/audio_3ai2/csv_files/1.csv\n",
            "{'time_stamp': '00:04:08.480 --> 00:04:13.280', 'transcription': \"about machine learning today, we're very good at writing learning algorithms that can automate\", 'DOC_ID': 42}\n",
            "************\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}